---
categories:
- ""
- ""
date: "2017-10-31T21:28:43-05:00"
description: ""
draft: false
image: movie.jpg
keywords: ""
slug: drinking
title: Drinking behavior, Movie ratings, Financial returns, German Polls

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="loading-necessary-packages" class="section level1">
<h1>Loading necessary packages:</h1>
<p>Code loads the necessary packages to analyze the data and present the charts. (Not included in HTML)</p>
</div>
<div id="where-do-people-drink-the-most-beer-wine-and-spirits" class="section level1">
<h1>Where Do People Drink The Most Beer, Wine And Spirits?</h1>
<p>Back in 2014, <a href="https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/">fivethiryeight.com</a> published an article on alcohol consumption in different countries. The data <code>drinks</code> is available as part of the <code>fivethirtyeight</code> package which will be used here.</p>
<p>We load the ‘fivethirtyeight’ package and drinks dataset with the code below:</p>
<pre class="r"><code>library(fivethirtyeight)
data(drinks)


# or download directly
alcohol_direct &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv&quot;)</code></pre>
<p>Following this we can skim and glimpse the data in order to see if there are any missing values we should worry about and to see the data and variable types.</p>
<pre class="r"><code>skim(alcohol_direct)</code></pre>
<table>
<caption>(#tab:glimpse_skim_data)Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">alcohol_direct</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">193</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">country</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">28</td>
<td align="right">0</td>
<td align="right">193</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">beer_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">106.16</td>
<td align="right">101.14</td>
<td align="right">0</td>
<td align="right">20.0</td>
<td align="right">76.0</td>
<td align="right">188.0</td>
<td align="right">376.0</td>
<td align="left">▇▃▂▂▁</td>
</tr>
<tr class="even">
<td align="left">spirit_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">80.99</td>
<td align="right">88.28</td>
<td align="right">0</td>
<td align="right">4.0</td>
<td align="right">56.0</td>
<td align="right">128.0</td>
<td align="right">438.0</td>
<td align="left">▇▃▂▁▁</td>
</tr>
<tr class="odd">
<td align="left">wine_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">49.45</td>
<td align="right">79.70</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">8.0</td>
<td align="right">59.0</td>
<td align="right">370.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">total_litres_of_pure_alcohol</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.72</td>
<td align="right">3.77</td>
<td align="right">0</td>
<td align="right">1.3</td>
<td align="right">4.2</td>
<td align="right">7.2</td>
<td align="right">14.4</td>
<td align="left">▇▃▅▃▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code>glimpse(alcohol_direct)</code></pre>
<pre><code>## Rows: 193
## Columns: 5
## $ country                      &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;And~
## $ beer_servings                &lt;dbl&gt; 0, 89, 25, 245, 217, 102, 193, 21, 261, 2~
## $ spirit_servings              &lt;dbl&gt; 0, 132, 0, 138, 57, 128, 25, 179, 72, 75,~
## $ wine_servings                &lt;dbl&gt; 0, 54, 14, 312, 45, 45, 221, 11, 212, 191~
## $ total_litres_of_pure_alcohol &lt;dbl&gt; 0.0, 4.9, 0.7, 12.4, 5.9, 4.9, 8.3, 3.8, ~</code></pre>
<p>The overview seems to have a comprehensive number of variables. However, some countries have zero total consumption, hence we believe the data is not comprehensive and complete. The variable types are numeric and characters for the country variable.</p>
<p>Below a plot is presented to show the top 25 beer drinking countries:</p>
<pre class="r"><code>alcohol_direct %&gt;% #loading the dataset
  slice_max(order_by = beer_servings, n=25) %&gt;% #sorting by top 25 countries
  ggplot(aes(x = beer_servings, y = reorder(country, beer_servings))) + #Choosing variables on each axis and sorting to have the highest countries on top.
  geom_col(fill = &quot;Orange&quot;) + #Making the graph orange
  labs(title = &quot;Namibia is the most beer-consuming country&quot;, #This line and the ones below add titles and axis
       subtitle = &quot;Beers consumed by country, top 25 countries woldwide, 2010&quot;, 
       x = &quot;Beer servings per person&quot;,
       y = &quot;Country&quot;,
       caption = &quot;https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/&quot;) + 
  theme_bw() + #Making a more simple theme
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/beer_plot-1.png" width="648" style="display: block; margin: auto;" />
From the graph it is evident that Namibia drinks the most beers per capita and that some countries drink up toward one beer per day per capita.</p>
<p>We now want to see the top 25 wine producing countries.</p>
<pre class="r"><code>alcohol_direct %&gt;% #Loading the dataset
  slice_max(order_by = wine_servings, n=25) %&gt;% #Sorting for top 25 countries
  ggplot(aes(x = wine_servings, y = reorder(country, wine_servings))) + #Choosing variable types and sorting the variables by servings
  geom_col(fill = &quot;Darkred&quot;) + #Making the bars dark red fitting with the wine theme
  labs(title = &quot;France is the most wine-consuming country&quot;, #This section adds titles
       subtitle = &quot;Wine consumed by country, top 25 countries worldwide, 2010&quot;, 
       x = &quot;Wine servings per person&quot;,
       y = &quot;Country&quot;,
       caption = &quot;https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/&quot;) + 
  theme_bw() + #Adding a more simple theme
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/wine_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>We see that especially France and Portugal drinks a lot of wine per capita also equivalent of a unit per day per person like the top beer drinking countries. There is a bit further span on the top 25 wine-drinking countries where the lowest countries are as low as around 160 compared to 240 for beer drinking countries.</p>
<p>Finally, we make a plot that shows the top 25 spirit consuming countries</p>
<pre class="r"><code>alcohol_direct %&gt;% #choosing the data
  slice_max(order_by = spirit_servings, n=25) %&gt;% #sorting by top 25 countries
  ggplot(aes(x = spirit_servings, y = reorder(country, spirit_servings))) + #plotting variables and reordering by top max on top
  geom_col(fill = &quot;Darkgreen&quot;) + #Making the plot dark green
  labs(title = &quot;Grenada is the most spirit-consuming country&quot;,#adding titles
       subtitle = &quot;Spirit consumed by country, top 25 countries worldwide, 2010&quot;, 
       x = &quot;Spirit servings per person&quot;,
       y = &quot;Country&quot;,
       caption = &quot;https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/&quot;) + 
  theme_bw() + 
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/spirit_plot-1.png" width="648" style="display: block; margin: auto;" />
Here we see that Grenada is the highest spirits drinking country but also the country with the most units for a single alcohol type with approximately 430 units.</p>
<p>While these three charts show the overall consumption of different types of alcoholic beverages, the top 25 countries differ highly by category because each country has a high preference for one certain type of alcohol. For the same reason, we cannot infer whether these countries are the highest alcohol-consuming countries overall. Furthermore, it seems that within all categories the top 25 countries drink approximately 150-370 servings. This is with the exception of Grenada where spirit servings are above 400 units. This could likely indicate a preference for spirits also could be due to a lack of availability of other alcohol types in the country.</p>
<p>In addition to the above, the graphs show units per person so we cannot infer which country has the highest total consumption of units. For example although Grenada has the highest number of units per person their total consumption can be lower than that of other countries because of the low population.</p>
<p>Furthermore, this data shows per capita consumption, however, it would be interesting to dive deeper into how this is segmented throughout the countries, i.e. if certain segments have much higher consumption than others.</p>
<p>The data is from 2010 and consumption might have changed since then.</p>
</div>
<div id="analysis-of-movies--imdb-dataset" class="section level1">
<h1>Analysis of movies- IMDB dataset</h1>
<p>We will look at a subset sample of movies, taken from the <a href="https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset">Kaggle IMDB 5000 movie dataset</a></p>
<p>The below code loads the dataset</p>
<pre class="r"><code>movies &lt;- read_csv(here::here(&quot;data&quot;, &quot;movies.csv&quot;))
glimpse(movies) # glimpse the dataset to understand the dataset further</code></pre>
<pre><code>## Rows: 2,961
## Columns: 11
## $ title               &lt;chr&gt; &quot;Avatar&quot;, &quot;Titanic&quot;, &quot;Jurassic World&quot;, &quot;The Avenge~
## $ genre               &lt;chr&gt; &quot;Action&quot;, &quot;Drama&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;~
## $ director            &lt;chr&gt; &quot;James Cameron&quot;, &quot;James Cameron&quot;, &quot;Colin Trevorrow~
## $ year                &lt;dbl&gt; 2009, 1997, 2015, 2012, 2008, 1999, 1977, 2015, 20~
## $ duration            &lt;dbl&gt; 178, 194, 124, 173, 152, 136, 125, 141, 164, 93, 1~
## $ gross               &lt;dbl&gt; 7.61e+08, 6.59e+08, 6.52e+08, 6.23e+08, 5.33e+08, ~
## $ budget              &lt;dbl&gt; 2.37e+08, 2.00e+08, 1.50e+08, 2.20e+08, 1.85e+08, ~
## $ cast_facebook_likes &lt;dbl&gt; 4834, 45223, 8458, 87697, 57802, 37723, 13485, 920~
## $ votes               &lt;dbl&gt; 886204, 793059, 418214, 995415, 1676169, 534658, 9~
## $ reviews             &lt;dbl&gt; 3777, 2843, 1934, 2425, 5312, 3917, 1752, 1752, 35~
## $ rating              &lt;dbl&gt; 7.9, 7.7, 7.0, 8.1, 9.0, 6.5, 8.7, 7.5, 8.5, 7.2, ~</code></pre>
<p>Besides the obvious variables of <code>title</code>, <code>genre</code>, <code>director</code>, <code>year</code>, and <code>duration</code>, the rest of the variables are as follows:</p>
<ul>
<li><code>gross</code> : The gross earnings in the US box office, not adjusted for inflation</li>
<li><code>budget</code>: The movie’s budget</li>
<li><code>cast_facebook_likes</code>: the number of facebook likes cast members received</li>
<li><code>votes</code>: the number of people who voted for (or rated) the movie in IMDB</li>
<li><code>reviews</code>: the number of reviews for that movie</li>
<li><code>rating</code>: IMDB average rating</li>
</ul>
<div id="we-wanna-further-use-our-data-import-inspection-and-cleaning-skills-to-answer-the-following" class="section level2">
<h2>We wanna further use our data import, inspection, and cleaning skills to answer the following:</h2>
<ul>
<li>Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?</li>
</ul>
<p>The below code helps us understand the dataset.</p>
<pre class="r"><code>skim(movies)</code></pre>
<table>
<caption>(#tab:skim movies)Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">movies</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">2961</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">title</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">83</td>
<td align="right">0</td>
<td align="right">2907</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">genre</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">11</td>
<td align="right">0</td>
<td align="right">17</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">director</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">32</td>
<td align="right">0</td>
<td align="right">1366</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="13%" />
<col width="6%" />
<col width="9%" />
<col width="6%" />
<col width="6%" />
<col width="4%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2.00e+03</td>
<td align="right">9.95e+00</td>
<td align="right">1920.0</td>
<td align="right">2.00e+03</td>
<td align="right">2.00e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.02e+03</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">duration</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.10e+02</td>
<td align="right">2.22e+01</td>
<td align="right">37.0</td>
<td align="right">9.50e+01</td>
<td align="right">1.06e+02</td>
<td align="right">1.19e+02</td>
<td align="right">3.30e+02</td>
<td align="left">▃▇▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">gross</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.81e+07</td>
<td align="right">7.25e+07</td>
<td align="right">703.0</td>
<td align="right">1.23e+07</td>
<td align="right">3.47e+07</td>
<td align="right">7.56e+07</td>
<td align="right">7.61e+08</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">budget</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.06e+07</td>
<td align="right">4.37e+07</td>
<td align="right">218.0</td>
<td align="right">1.10e+07</td>
<td align="right">2.60e+07</td>
<td align="right">5.50e+07</td>
<td align="right">3.00e+08</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">cast_facebook_likes</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.24e+04</td>
<td align="right">2.05e+04</td>
<td align="right">0.0</td>
<td align="right">2.24e+03</td>
<td align="right">4.60e+03</td>
<td align="right">1.69e+04</td>
<td align="right">6.57e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">votes</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.09e+05</td>
<td align="right">1.58e+05</td>
<td align="right">5.0</td>
<td align="right">1.99e+04</td>
<td align="right">5.57e+04</td>
<td align="right">1.33e+05</td>
<td align="right">1.69e+06</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">reviews</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.03e+02</td>
<td align="right">4.94e+02</td>
<td align="right">2.0</td>
<td align="right">1.99e+02</td>
<td align="right">3.64e+02</td>
<td align="right">6.31e+02</td>
<td align="right">5.31e+03</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">rating</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.39e+00</td>
<td align="right">1.05e+00</td>
<td align="right">1.6</td>
<td align="right">5.80e+00</td>
<td align="right">6.50e+00</td>
<td align="right">7.10e+00</td>
<td align="right">9.30e+00</td>
<td align="left">▁▁▆▇▁</td>
</tr>
</tbody>
</table>
<p>Although we have 2907 titles, we have 2961 rows for the data. Therefore, we believe there are duplicate entries in the dataset. There are no missing values according to the skim function.</p>
<ul>
<li>Produce a table with the count of movies by genre, ranked in descending order</li>
</ul>
<pre class="r"><code>genre &lt;- movies %&gt;% # choosing the movie dataset
  group_by(genre) %&gt;% #grouping by genre
  summarize(Movie_count = count(genre)) %&gt;%  #Counting the number of movies in each genre
  arrange(desc(Movie_count)) #Sorting the dataset descending by movie count
genre</code></pre>
<pre><code>## # A tibble: 17 x 2
##    genre       Movie_count
##    &lt;chr&gt;             &lt;int&gt;
##  1 Comedy              848
##  2 Action              738
##  3 Drama               498
##  4 Adventure           288
##  5 Crime               202
##  6 Biography           135
##  7 Horror              131
##  8 Animation            35
##  9 Fantasy              28
## 10 Documentary          25
## 11 Mystery              16
## 12 Sci-Fi                7
## 13 Family                3
## 14 Musical               2
## 15 Romance               2
## 16 Western               2
## 17 Thriller              1</code></pre>
<ul>
<li>We then produce a table with the average gross earning and budget (<code>gross</code> and <code>budget</code>) by genre. We calculate a variable <code>return_on_budget</code> which shows how many $ did a movie make at the box office for each $ of its budget. We also rank genres by this <code>return_on_budget</code> in descending order</li>
</ul>
<pre class="r"><code>genre &lt;- movies %&gt;% #choosing the movie dataset
  group_by(genre) %&gt;% #grouping by genre
  summarise(average_gross = mean(gross), #adding the mean gross revenue  
            average_budget = mean(budget)) %&gt;% #adding the mean budget 
  mutate(return_on_budget = average_gross/average_budget) %&gt;% # calculating the revenue to budget ratio
  arrange(desc(return_on_budget)) #sorting descending
genre</code></pre>
<pre><code>## # A tibble: 17 x 4
##    genre       average_gross average_budget return_on_budget
##    &lt;chr&gt;               &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;
##  1 Musical         92084000        3189500          28.9    
##  2 Family         149160478.      14833333.         10.1    
##  3 Western         20821884        3465000           6.01   
##  4 Documentary     17353973.       5887852.          2.95   
##  5 Horror          37713738.      13504916.          2.79   
##  6 Fantasy         42408841.      17582143.          2.41   
##  7 Comedy          42630552.      24446319.          1.74   
##  8 Mystery         67533021.      39218750           1.72   
##  9 Animation       98433792.      61701429.          1.60   
## 10 Biography       45201805.      28543696.          1.58   
## 11 Adventure       95794257.      66290069.          1.45   
## 12 Drama           37465371.      26242933.          1.43   
## 13 Crime           37502397.      26596169.          1.41   
## 14 Romance         31264848.      25107500           1.25   
## 15 Action          86583860.      71354888.          1.21   
## 16 Sci-Fi          29788371.      27607143.          1.08   
## 17 Thriller            2468         300000           0.00823</code></pre>
<p>We see that musical and family have the highest return on budget. For musical, this might be because of its low average budget.</p>
<ul>
<li>We then produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. We show the total gross amount, the mean, median, and standard deviation per director.</li>
</ul>
<pre class="r"><code>director &lt;- movies %&gt;% #choosing the dataset
  group_by(director) %&gt;% #filtering by director
  summarise(mean_gross = mean(gross), #adding the mean gross revenue column
            median_gross = median(gross), #adding the median gross revenue column
            SD_gross = sd(gross), #adding the SD on gross revenue column
            total_gross = sum(gross)) %&gt;% #adding the total gross revenue column
    slice_max(order_by = total_gross, n=15) #choosing the top 15 directors by total gross revenue
director #printing the table</code></pre>
<pre><code>## # A tibble: 15 x 5
##    director          mean_gross median_gross   SD_gross total_gross
##    &lt;chr&gt;                  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;
##  1 Steven Spielberg  174524422.   164435221  101421051.  4014061704
##  2 Michael Bay       171634041.   138396624  127161579.  2231242537
##  3 Tim Burton        129454718.    76519172  108726924.  2071275480
##  4 Sam Raimi         201460090.   234903076  162126632.  2014600898
##  5 James Cameron     318287652.   175562880. 309171337.  1909725910
##  6 Christopher Nolan 226653447    196667606. 187224133.  1813227576
##  7 George Lucas      348283696    380262555  146193880.  1741418480
##  8 Robert Zemeckis   124562239.   100853835   91300279.  1619309108
##  9 Clint Eastwood     72543216.    46700000   75487408.  1378321100
## 10 Francis Lawrence  271700394.   281666058  135437020.  1358501971
## 11 Ron Howard        111332341    101587923   81933761.  1335988092
## 12 Gore Verbinski    189942999.   123207194  154473822.  1329600995
## 13 Andrew Adamson    284361730    279680930. 120895765.  1137446920
## 14 Shawn Levy        102704635.    85463309   65484773.  1129750988
## 15 Ridley Scott       80632686.    47775715   68812285.  1128857598</code></pre>
<p>Surprisingly, Steven Spielberg has much higher total gross revenue than other directors and almost twice as much as the 2nd highest grossing director, Michael Bay.</p>
<ul>
<li>Finally, ratings. We produce a table that describes how ratings are distributed by genre. We show the mean, but also, min, max, median, SD and a histogram that visually shows how ratings are distributed.</li>
</ul>
<pre class="r"><code>Rating &lt;- movies %&gt;% #Choosing the dataset and adding variable Rating
  group_by(genre) %&gt;% #Grouping by Genre
  summarise(mean = mean(rating),#Adding average
            min = min(rating), #Adding min
            max = max(rating), #Adding max
            median = median(rating), #Adding median
            SD = sd(rating)) %&gt;% #Adding Standard Deviation
  arrange(desc(mean)) #aRranging by mean
ggplot(Rating, aes(x = mean, y = reorder(genre, mean), fill = mean)) + #Plotting the rating sorted by mean rating
  geom_col() + #Choosing a column plot
  theme_bw() + #simplifying the theme
  labs(title = &quot;The Biography and Crime genres have the highest average rating&quot;, #Below rows add useful titles
       subtitle = &quot;Movie ratings by genre&quot;, 
       x = &quot;Average rating&quot;, 
       y = &quot;Genre&quot;, 
       caption = &quot;https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset&quot;) + 
  guides(fill=FALSE) + 
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/movie%20ratings-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>Rating #This prints the table</code></pre>
<pre><code>## # A tibble: 17 x 6
##    genre        mean   min   max median     SD
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 Biography    7.11   4.5   8.9   7.2   0.760
##  2 Crime        6.92   4.8   9.3   6.9   0.849
##  3 Mystery      6.86   4.6   8.5   6.9   0.882
##  4 Musical      6.75   6.3   7.2   6.75  0.636
##  5 Drama        6.73   2.1   8.8   6.8   0.917
##  6 Documentary  6.66   1.6   8.5   7.4   1.77 
##  7 Sci-Fi       6.66   5     8.2   6.4   1.09 
##  8 Animation    6.65   4.5   8     6.9   0.968
##  9 Romance      6.65   6.2   7.1   6.65  0.636
## 10 Adventure    6.51   2.3   8.6   6.6   1.09 
## 11 Family       6.5    5.7   7.9   5.9   1.22 
## 12 Action       6.23   2.1   9     6.3   1.03 
## 13 Fantasy      6.15   4.3   7.9   6.45  0.959
## 14 Comedy       6.11   1.9   8.8   6.2   1.02 
## 15 Horror       5.83   3.6   8.5   5.9   1.01 
## 16 Western      5.7    4.1   7.3   5.7   2.26 
## 17 Thriller     4.8    4.8   4.8   4.8  NA</code></pre>
<p>Interestingly, Biography and Crime have the highest average rating. In contrast, western and thrillers rank poorly.</p>
</div>
<div id="we-now-use-ggplot-to-answer-the-following" class="section level2">
<h2>We now Use <code>ggplot</code> to answer the following</h2>
<ul>
<li>We examine the relationship between <code>gross</code> and <code>cast_facebook_likes</code>. We produce a scatterplot and discuss whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office.</li>
</ul>
<pre class="r"><code># Plotting a graph with both variables to study the relationship

ggplot(movies, aes(cast_facebook_likes,gross))+
  geom_point(alpha = 0.3, color = &quot;lightblue&quot;)+
  scale_x_log10()+ # Using log scale to fit smoother the data with outliers
  scale_y_log10()+ # Using log scale to fit smoother the data with outliers
  
  # We see that variables are positively correlated and add the corresponding title to reflect this finding
  # Additionally, we add a subtitle on what we are doing, name the axes, and show the data source in the caption
  
  labs(title = &quot;Cast facebook likes is positively correlated to gross revenue&quot;,
       subtitle=&quot;Examining the relationship between gross revenue and the number of facebook likes that the cast has received&quot;,
       x=&quot;Cast number of facebook likes received&quot;,
       y=&quot;Gross revenue of movies&quot;,
       caption = &quot;https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset&quot;)+
  
  # Adding a linear regression line and setting a bw theme
  
  stat_smooth(method = &quot;lm&quot;) +
  theme_bw()+
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/gross_on_fblikes-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>The number of facebook likes received is positively correlated with the gross revenue of the movies as is evident from the chart and regression showing facebook likes on the X-axis and gross revenue on the Y-axis.</p>
<ul>
<li>We now examine the relationship between <code>gross</code> and <code>budget</code>. We produce a scatterplot and discuss whether budget is likely to be a good predictor of how much money a movie will make at the box office.</li>
</ul>
<pre class="r"><code># Plotting a graph with both variables to study the relationship

ggplot(movies, aes(budget,gross))+
  geom_point(alpha = 0.3, color = &quot;lightblue&quot;)+
  scale_x_log10()+ # Using log scale to fit smoother the data with outliers
  scale_y_log10()+ # Using log scale to fit smoother the data with outliers
  
  # We see that variables are positively correlated and add the corresponding title to reflect this finding
  # Additionally, we add a subtitle on what we are doing, name the axes, and show the data source in the caption  
  
  labs(title = &quot;Gross revenue is positively correlated to movie budget&quot;,
       subtitle=&quot;Examining the relationship between gross revenue and movie budget&quot;,
       x=&quot;Movie budget&quot;,
       y=&quot;Gross revenue of movies&quot;,
       caption = &quot;https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset&quot;)+
    
  # Adding a linear regression line and setting a bw theme
  
  stat_smooth(method = &quot;lm&quot;) +
  theme_bw()+
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/gross_on_budget-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Gross revenue of movies is positively related to the associated budget of the movies. Consequently movie budget is a good predictor of gross revenue.</p>
<ul>
<li>We now examine the relationship between <code>gross</code> and <code>rating</code>. We produce a scatterplot, faceted by <code>genre</code> and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. We also check if there is is anything strange in this dataset.</li>
</ul>
<pre class="r"><code># Plotting a graph with both variables to study the relationship

ggplot(movies, aes(x = rating, y = gross, color = genre)) + 
  geom_point(alpha = 0.3) + 
  
  # Adding a linear regression line
  
  stat_smooth(method = &quot;lm&quot;) + 
  
  # Dividing graphs based on genre to evaluate the relationship for every genre
  
  facet_wrap(&quot;genre&quot;, scales = &quot;free&quot;, ncol = 5) + 
  
  # Adjusting the scale to show everything in mn on y-axis
  
  scale_y_continuous(labels = scales::label_number_si())+
  theme_bw() +
  
  # Setting a bw theme (above), removing the legend, adding title on the degree of relationship
  # Outlining the subtitle to reflect the purpose of the graphs, naming axes, and adding a caption with the data source
  
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Most but not all genres have a positive relationship between gross revenue and ratings&quot;,
       subtitle=&quot;Examining the relationship between gross revenue and rating of movies by genre&quot;,
       x=&quot;Movie rating&quot;,
       y=&quot;Gross revenue of movie&quot;, 
       caption = &quot;https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset&quot;) + 
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/gross_on_rating-1.png" width="648" style="display: block; margin: auto;" />
For most genres rating is a good predictor of gross revenue. However, there are some general genres with only a few datapoints to infer the relationship. Therefore we cannot determine this relationship for these genres with high confidence. The genres include Thriller, Western, Musical, and Romance. In addition its interesting to see that the higher ratings documentaries have the lower gross revenue they have, i.e. a negative relationship between the two.</p>
</div>
</div>
<div id="returns-of-financial-stocks" class="section level1">
<h1>Returns of financial stocks</h1>
<blockquote>
<p>Useful material can be found on <a href="https://mfa2022.netlify.app/reference/finance_data/">finance data sources</a>.</p>
</blockquote>
<p>We will use the <code>tidyquant</code> package to download historical data of stock prices, calculate returns, and examine the distribution of returns.
he file <code>nyse.csv</code> contains 508 stocks listed on the NYSE, their ticker <code>symbol</code>, <code>name</code>, the IPO (Initial Public Offering) year, and the sector and industry the company is in.</p>
<pre class="r"><code>nyse &lt;- read_csv(here::here(&quot;data&quot;,&quot;nyse.csv&quot;)) #loading the data
glimpse(nyse) #Glimsing the data to understand the dataset</code></pre>
<pre><code>## Rows: 508
## Columns: 6
## $ symbol        &lt;chr&gt; &quot;MMM&quot;, &quot;ABB&quot;, &quot;ABT&quot;, &quot;ABBV&quot;, &quot;ACN&quot;, &quot;AAP&quot;, &quot;AFL&quot;, &quot;A&quot;, &quot;~
## $ name          &lt;chr&gt; &quot;3M Company&quot;, &quot;ABB Ltd&quot;, &quot;Abbott Laboratories&quot;, &quot;AbbVie ~
## $ ipo_year      &lt;chr&gt; &quot;n/a&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;2012&quot;, &quot;2001&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;1999~
## $ sector        &lt;chr&gt; &quot;Health Care&quot;, &quot;Consumer Durables&quot;, &quot;Health Care&quot;, &quot;Heal~
## $ industry      &lt;chr&gt; &quot;Medical/Dental Instruments&quot;, &quot;Electrical Products&quot;, &quot;Ma~
## $ summary_quote &lt;chr&gt; &quot;https://www.nasdaq.com/symbol/mmm&quot;, &quot;https://www.nasdaq~</code></pre>
<p>We see a tabe of characters such as ticker, IPO year and more of different companies on the NYSE</p>
<p>Based on this dataset, we create a table and a bar plot that shows the number of companies per sector, in descending order</p>
<pre class="r"><code>comp_per_sec &lt;- nyse %&gt;% #choosing the correct dataset
  group_by(sector) %&gt;% #grouping by sector
  summarize(Number_of_companies = count(sector)) %&gt;% #counting the number of companies in each sector
  arrange(desc(Number_of_companies)) #arranging the dataset descending
comp_per_sec</code></pre>
<pre><code>## # A tibble: 12 x 2
##    sector                Number_of_companies
##    &lt;chr&gt;                               &lt;int&gt;
##  1 Finance                                97
##  2 Consumer Services                      79
##  3 Public Utilities                       60
##  4 Capital Goods                          45
##  5 Health Care                            45
##  6 Energy                                 42
##  7 Technology                             40
##  8 Basic Industries                       39
##  9 Consumer Non-Durables                  31
## 10 Miscellaneous                          12
## 11 Transportation                         10
## 12 Consumer Durables                       8</code></pre>
<pre class="r"><code>ggplot(comp_per_sec,aes(x=Number_of_companies,y=reorder(sector,Number_of_companies)))+ #Plotting the dataset with companies in each sector
  geom_col(fill=&quot;pink&quot;)+ #coloring the dataset pink
  labs(title=&quot;Number of companies per sector in the NYSE&quot;, #adding relevant titles
       x=&quot;Number of companies&quot;,
       y=&quot;Sector&quot;, 
       caption = &quot;https://mfa2022.netlify.app/reference/finance_data/&quot;)+
  theme_bw() + #adding a minimalistic theme. 
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/companies_per_sector-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>We see that the most common company sectors in the NYSE are Finance and Consumer services and the least common sectors are Transportation and Consumer Durables.</p>
<p>Next, we look at the <a href="https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average">Dow Jones Industrial Aveareg (DJIA)</a> stocks and their ticker symbols and download some data. Besides the thirty stocks that make up the DJIA, we also add <code>SPY</code> which is an SP500 ETF (Exchange Traded Fund). The following code</p>
<pre class="r"><code>djia_url &lt;- &quot;https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average&quot;


#get tables that exist on URL
tables &lt;- djia_url %&gt;% 
  read_html() %&gt;% 
  html_nodes(css=&quot;table&quot;)


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia &lt;- map(tables, . %&gt;% 
               html_table(fill=TRUE)%&gt;% 
               clean_names())


# constituents
table1 &lt;- djia[[2]] %&gt;% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains &quot;NYSE*&quot;, the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, &quot;NYSE*&quot;),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers &lt;- table1 %&gt;% 
  select(ticker) %&gt;% 
  pull() %&gt;% # pull() gets them as a sting of characters
  c(&quot;SPY&quot;) # and lets us add SPY, the SP500 ETF</code></pre>
<p>Now let us downlaod prices for all 30 DJIA consituents and the SPY ETF that tracks SP500 since January 1, 2020</p>
<pre class="r"><code># Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks &lt;- tickers %&gt;% 
  tq_get(get  = &quot;stock.prices&quot;,
         from = &quot;2000-01-01&quot;,
         to   = Sys.Date()) %&gt;% # Sys.Date() returns today&#39;s price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame</code></pre>
<pre><code>## Rows: 162,018
## Columns: 8
## Groups: symbol [31]
## $ symbol   &lt;chr&gt; &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;~
## $ date     &lt;date&gt; 2000-01-03, 2000-01-04, 2000-01-05, 2000-01-06, 2000-01-07, ~
## $ open     &lt;dbl&gt; 48.0, 46.4, 45.6, 47.2, 50.6, 50.2, 50.4, 51.0, 50.7, 50.4, 4~
## $ high     &lt;dbl&gt; 48.2, 47.4, 48.1, 51.2, 51.9, 51.8, 51.2, 51.8, 50.9, 50.5, 4~
## $ low      &lt;dbl&gt; 47.0, 45.3, 45.6, 47.2, 50.0, 50.0, 50.2, 50.4, 50.2, 49.5, 4~
## $ close    &lt;dbl&gt; 47.2, 45.3, 46.6, 50.4, 51.4, 51.1, 50.2, 50.4, 50.4, 49.7, 4~
## $ volume   &lt;dbl&gt; 2173400, 2713800, 3699400, 5975800, 4101200, 3863800, 2357600~
## $ adjusted &lt;dbl&gt; 27.2, 26.1, 26.9, 29.0, 29.6, 29.4, 28.9, 29.0, 29.0, 28.6, 2~</code></pre>
<p>Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.</p>
<pre class="r"><code>#calculate daily returns
myStocks_returns_daily &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;daily&quot;, 
               type       = &quot;log&quot;,
               col_rename = &quot;daily_returns&quot;,
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;monthly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;monthly_returns&quot;,
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual &lt;- myStocks %&gt;%
  group_by(symbol) %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;yearly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;yearly_returns&quot;,
               cols = c(nested.col))</code></pre>
<p>We now create a table where we summarise monthly returns for each of the stocks and <code>SPY</code>; min, max, median, mean, SD.</p>
<pre class="r"><code>#First we pick the monthly returns dataset
summarise_monthly_returns &lt;- myStocks_returns_monthly %&gt;% 
  
  #Grouping by ticker (company)
  group_by(symbol) %&gt;% 
  
  #Adding Min, Max, Median, Mean, and SD
  summarize(min_return=min(monthly_returns),
            max_return=max(monthly_returns),
            median_return=median(monthly_returns),
            mean_return=mean(monthly_returns),
            SD=sd(monthly_returns)) %&gt;% 
  
  #Arranging by highest mean return
  arrange(desc(mean_return))


#Printing the table
summarise_monthly_returns</code></pre>
<pre><code>## # A tibble: 31 x 6
##    symbol min_return max_return median_return mean_return     SD
##    &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;
##  1 AAPL       -0.577      0.454        0.0352      0.0269 0.115 
##  2 CRM        -0.360      0.403        0.0205      0.0262 0.110 
##  3 V          -0.196      0.338        0.0256      0.0200 0.0660
##  4 UNH        -0.306      0.266        0.0232      0.0192 0.0699
##  5 NKE        -0.375      0.396        0.0169      0.0163 0.0758
##  6 DOW        -0.276      0.255        0.0325      0.0156 0.109 
##  7 CAT        -0.353      0.350        0.0141      0.0144 0.0898
##  8 BA         -0.458      0.459        0.0165      0.0126 0.0925
##  9 MSFT       -0.344      0.408        0.0177      0.0115 0.0820
## 10 GS         -0.275      0.312        0.0163      0.0111 0.0926
## # ... with 21 more rows</code></pre>
<p>We now plot a density plot, using <code>geom_density()</code>, for each of the stocks</p>
<pre class="r"><code>#First we choose the monthly return data and plot return for each company
ggplot(myStocks_returns_monthly, aes(x=monthly_returns, fill = symbol))+
  
  #We then add a density curve with a black outline
  geom_density(color = &quot;black&quot;)+
  
  #This is used to seperate each company
  facet_wrap(~symbol, ncol = 2)+
  scale_x_continuous(labels = scales::percent) +
  
  #Adding useful titles
  labs(title = &quot;Return observations by company since January 2000&quot;, 
       x = &quot;Monthly return&quot;,
       y = &quot;Frequency&quot;) + 
  
  #Removeing legens
  guides(fill=FALSE) + 
  
  #Adding a simplified theme
  theme_bw() + 
  
  #Adding a line at x=0 to make the graphs more readable and understand if their observations are above or below 0% return. 
  geom_vline(xintercept = 0, alpha = 0.3, linetype = &quot;dashed&quot;, labs = &quot;0%&quot;)  +
  annotate(&quot;text&quot;, label = &quot;0%&quot;, x = 0.10, y = 11, size = 4, colour = &quot;grey&quot;)+
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/density_monthly_returns-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>From this plot, we can conclude that AAPL is the riskiest stock because it has the highest standard deviation (based on skim function) and low kurtosis. ON the other hand, SPY is the least risky stock as it has the lowest standard deviation with high kurtosis. The reason for Apple to be the riskiest as it is an IT company and, probably, has the highest beta to the market. Recent market trend shows that tech companies are the most vulnerable to changes on the market with higher frequency of extremely high and low returns over time. The S&amp;P Index has the lowest degree of risk since it is diversified with multiple stocks inside of the index, hence the deviation of it is low</p>
<p>Finally, we make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. We use the <code>ggrepel::geom_text_repel()</code> to label each stock</p>
<pre class="r"><code>#Using our previous table to choose mean return versus standard deviation
ggplot(summarise_monthly_returns,aes(y=mean_return,x=SD,label=symbol, size=3))+
  
  #Adding it in a scatter plot
  geom_point(color=&quot;red&quot;, alpha = 0.4)+
  
  #Adding titles
    labs(title = &quot;Expected monthly return of a stock based on its risk&quot;, 
       x = &quot;Standard Deviation&quot;,
       y = &quot;Mean Returns&quot;) +
  
  #Y axis in %
  scale_y_continuous(labels = scales::percent) +
  
  #Adding a trendline
  stat_smooth(method = &quot;lm&quot;, size=1, se = FALSE) +
  
  #Removing legend
  guides(size=FALSE) +
  
  #Adding labels
  ggrepel::geom_text_repel()+
  
  #Simplifying theme
  theme_bw() +
  
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/risk_return_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Based on the plot and regression line we have designed, we observe that CSCO had a low mean return with a high standard deviation. We believe that any stock positioned below our regression line should be considered as a high risk low expected return equities. Furthermore, WMT and CSCO share the same level of expected returns while CSCO has a higher standard deviation compared to WMT, hence we would prefer Walmart stock over Cisco one based on this data of past returns</p>
</div>
<div id="is-inflation-transitory" class="section level1">
<h1>Is inflation transitory?</h1>
<blockquote>
<p>Useful the material on <a href="https://mfa2022.netlify.app/reference/finance_data/#data-from-the-federal-reserve-economic-data-with-tidyquant">downloading economic data from the FRED</a>.</p>
</blockquote>
<p>A recent study by the Bank for International Settlements (BIS) claimed that the <a href="https://www.bloomberg.com/news/articles/2021-09-20/current-inflation-spike-is-just-transitory-new-bis-study-argues">Current Inflation Spike Is Just Transitory</a>. As the article says,</p>
<blockquote>
<p>The surge in inflation seen across major economies is probably short lived because it’s confined to just a few sectors of the economy, according to the Bank for International Settlements.</p>
</blockquote>
<blockquote>
<p>New research by the BIS’s Claudio Borio, Piti Disyatat, Egon Zakrajsek and Dora Xia adds to one of the hottest debates in economics – how long the current surge in consumer prices will last. Both Federal Reserve Chair Jerome Powell and his euro-area counterpart Christine Lagarde have said the pickup is probably transitory, despite a snarled global supply chain and a spike in energy prices.</p>
</blockquote>
<p>We here have to download data for CPI and the 10 year bill and produce the following graph -&gt; We downloaded the data for for CPI and the 10-year T-Bill and produce graphs on the relation between these variables</p>
<p>The relevant indicators we will use the replicate that chart can be found in the following links -&gt; We used the following indicators to replicate the chart</p>
<ul>
<li><a href="https://fred.stlouisfed.org/series/CPIAUCSL">Consumer Price Index for All Urban Consumers: All Items in U.S. City Average</a></li>
<li><a href="https://fred.stlouisfed.org/series/GS10">10-Year Treasury Constant Maturity Rate</a></li>
</ul>
<pre class="r"><code>cpi  &lt;-   tq_get(&quot;CPIAUCSL&quot;, get = &quot;economic.data&quot;,
                       from = &quot;1980-01-01&quot;) %&gt;% 
  rename(cpi = symbol,  # FRED data is given as &#39;symbol&#39; and &#39;price&#39;
         rate = price) %&gt;% # we rename them to what they really are, e.g., cpi and rate
  
  # calculate yearly change in CPI by dividing current month by same month a year (or 12 months) earlier, minus 1
  mutate(cpi_yoy_change = rate/lag(rate, 12) - 1)

ten_year_monthly  &lt;-   tq_get(&quot;GS10&quot;, get = &quot;economic.data&quot;,
                       from = &quot;1980-01-01&quot;) %&gt;% 
  rename(ten_year = symbol,
         yield = price) %&gt;% 
  mutate(yield = yield / 100) # original data is not given as, e.g., 0.05, but rather 5, for five percent

# we have the two dataframes-- we now need to join them, and we will use left_join()
# base R has a function merge() that does the same, but it&#39;s slow, so please don&#39;t use it

mydata &lt;- 
  cpi %&gt;% 
  left_join(ten_year_monthly, by=&quot;date&quot;) %&gt;% 
  mutate(
    year = year(date), # using lubridate::year() to generate a new column with just the year
    month = month(date, label = TRUE),
    decade=case_when(
      year %in% 1980:1989 ~ &quot;1980s&quot;,
      year %in% 1990:1999 ~ &quot;1990s&quot;,
      year %in% 2000:2009 ~ &quot;2000s&quot;,
      year %in% 2010:2019 ~ &quot;2010s&quot;,
      TRUE ~ &quot;2020s&quot;
      )
  )


#First we would like to filter out data from 1980 as those datapoints do not have YOY change (there is no data from 1979 to calculate this)
mydatafiltered &lt;- mydata %&gt;% 
  filter(year &gt; &quot;1980&quot;)


#We now plot the data with CPI and Yield colored by decate
ggplot(mydatafiltered, aes(x = cpi_yoy_change, y = yield, color = decade)) +
  
  #Using a scatterplot
geom_point() + 
  
  #Adding labels in %
  scale_x_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  
  #Categorizing by decade and i 1 column with different axis scales
  facet_wrap(&quot;decade&quot;, ncol = 1, scales = &quot;free&quot;) + 
  
  #Adding a minimal theme
  theme_bw() + 
  
  #Giving each category a trendline
  stat_smooth(se = FALSE, method = &quot;lm&quot;) + 
  
  #Removing Legends
  theme(legend.position = &quot;none&quot;) + 
  
  #Adding useful titles
  labs(title = &quot;How are CPI and 10-year yield related?&quot;, 
       x = &quot;CPI Yearly Change&quot;, 
       y = &quot;10-Year Treasury Constant Maturity Rate&quot;) + 
  
  #Adding the labels with months and year in a small size
  ggrepel::geom_text_repel(aes(label = format(date,&quot;%B %Y&quot;)), size = 2) +
  
  #Choosing a proper aspect ratio
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/get_cpi_10Year_yield-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="challenge-2-opinion-polls-for-the-2021-german-elections" class="section level1">
<h1>Challenge 2: Opinion polls for the 2021 German elections</h1>
<p>The Guardian newspaper has an <a href="https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor">election poll tracker for the upcoming German election</a>.</p>
<p>WE will reproduce the graph made in the guardian</p>
<p>The following code will scrape the wikipedia page and import the table in a dataframe. Afterward the graph is produced</p>
<pre class="r"><code>url &lt;- &quot;https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election&quot;

# similar graphs and analyses can be found at 
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel


# get tables that exist on wikipedia page 
tables &lt;- url %&gt;% 
  read_html() %&gt;% 
  html_nodes(css=&quot;table&quot;)


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls &lt;- map(tables, . %&gt;% 
             html_table(fill=TRUE)%&gt;% 
             janitor::clean_names())


# list of opinion polls
german_election_polls &lt;- polls[[1]] %&gt;% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %&gt;%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )




#remember this fig.width=25, fig.height=12 in the{}
#use the average support rate of the same day to build up a new dataset
df_vote &lt;- german_election_polls %&gt;% 
  group_by(end_date) %&gt;% 
  summarize(CDU= mean(union)/100,
            SPD= mean(spd)/100,
            AFD=mean(af_d)/100,
            FDP= mean(fdp)/100,
            Linke=mean(linke)/100,
            Grune=mean(grune)/100)
df_vote</code></pre>
<pre><code>## # A tibble: 146 x 7
##    end_date     CDU   SPD   AFD    FDP  Linke Grune
##    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1 2021-01-04 0.365 0.155  0.1  0.0675 0.0775 0.18 
##  2 2021-01-05 0.36  0.15   0.1  0.06   0.09   0.18 
##  3 2021-01-06 0.35  0.14   0.1  0.07   0.07   0.21 
##  4 2021-01-08 0.36  0.14   0.08 0.07   0.08   0.2  
##  5 2021-01-11 0.36  0.15   0.1  0.075  0.08   0.18 
##  6 2021-01-13 0.36  0.15   0.1  0.07   0.08   0.18 
##  7 2021-01-14 0.37  0.15   0.1  0.05   0.08   0.2  
##  8 2021-01-15 0.35  0.15   0.09 0.06   0.08   0.2  
##  9 2021-01-17 0.35  0.15   0.09 0.07   0.08   0.19 
## 10 2021-01-18 0.352 0.145  0.11 0.0875 0.0775 0.172
## # ... with 136 more rows</code></pre>
<pre class="r"><code>#form a date breaks for x-ray
library(scales)
datebreaks &lt;- seq( as.Date(&quot;2021-04-01&quot;),as.Date(&quot;2021-07-01&quot;), by=&quot;3 month&quot;) 

#Adding colors for Labs
Party &lt;- c(&quot;Union&quot; = &quot;Black&quot;, &quot;spd&quot; = &quot;Firebrick&quot;, &quot;AF&quot; = &quot;Deepskyblue3&quot;, &quot;FDP&quot; = &quot;Goldenrod1&quot;, &quot;Linke&quot; = &quot;Darkmagenta&quot;, &quot;Grune&quot; = &quot;Forestgreen&quot;)

#plot - we choose election polls and dates as x and y variables
ggplot(data = german_election_polls, aes(x= end_date))+
  
  #and add a minimal theme
  theme_bw()+
  
  #We then plot each part of the data separately for each party. This includes a scatter plot and trend line
  geom_point(aes(y = union), color = &quot;Black&quot;, size=2, alpha = 0.2) +
  geom_smooth(aes(y=union, color = &quot;Union&quot;, method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  geom_point(aes(y = spd), color = &quot;Firebrick&quot;, size=2,  alpha= 0.2)+
  geom_smooth(aes(y=spd, color = &quot;spd&quot;, method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  geom_point(aes(y = af_d), color=&quot;Deepskyblue3&quot;, size=2, alpha= 0.2)+
  geom_smooth(aes(y=af_d, color = &quot;AF&quot;,method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  geom_point(aes(y = fdp), color=&quot;Goldenrod1&quot;, size=2, alpha= 0.2)+
  geom_smooth(aes(y=fdp,color = &quot;FDP&quot;, method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  geom_point(aes(y = linke), color=&quot;Darkmagenta&quot;, size=2, alpha= 0.2)+
  geom_smooth(aes(y=linke, color = &quot;Linke&quot;, method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  geom_point(aes(y = grune), color=&quot;Forestgreen&quot;, size=2, alpha= 0.2)+
  geom_smooth(aes(y=grune, color = &quot;Grune&quot;, method=&quot;lm&quot;, se = FALSE), level = 0, span=0.05)+
  
  #Scaling colors
  scale_color_manual(values = Party) +
  
  #We then remove the legend, add a title inside in the middle of the plot
  theme(legend.position = c(0.9, 0.9),  #Choosing legend position
        legend.background = element_rect(fill=&quot;white&quot;, linetype = &quot;solid&quot;, color = &quot;black&quot;),  #Background white and black outline
        plot.title = element_text(hjust =0.5),  #Title position
        plot.caption = element_text(hjust=0))+ 
  
  #Removing the background filler on the legend
  guides(color=guide_legend(override.aes=list(fill=NA))) + 
  
  #Adding a title to the legend
  labs(color=&#39;Political Party&#39;)  + 
  
  #Adding label descriptions
  labs(title = &quot;German election poll tracker&quot;,
       x = &quot;Month (2021 YTD)&quot;,
       y = &quot;Vote distribution (%)&quot;,
       caption = &quot;Source: wahlrecht.de, last updated 25 Sep 2021&quot;)+
  
  #Removing vertical grid lines
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())+
  
  NULL</code></pre>
<p><img src="/blogs2/blog1_files/figure-html/scrape_wikipedia_polling_data-1.png" width="648" style="display: block; margin: auto;" />
We see from this chart that SPD have the highest number of votes currently and Link has the lowest (as of 21st of September.)</p>
</div>
